{"version":"NotebookV1","origId":1092620887341761,"name":"06_serve_features_and_model","language":"python","commands":[{"version":"CommandV1","origId":4523734794182324,"guid":"f812f6a3-b960-44c4-93dd-9e9ec0b6f6bb","subtype":"command","commandType":"auto","position":0.5,"command":"%md\n# Churn Prediction Realtime Inference\n\nWe have just seen how to get predictions in batches. Now, we will deploy the features and model to make real-time predictions via a REST API call. Customer application teams can embed this predictive capability into customer-facing applications and apply a retention strategy for customers predicted to churn as they interact with the application.\n\nBecause the predictions are to be made in a customer-facing application as the customer interacts with it, they have to be returned with low latency.\n\n\nTo serve the features and model, we will:\n\nMake the features available for low-latency retrieval by the model through Databricks' online tables\nDeploy the registered model from Unity Catalog to a Model Serving endpoint for low latency serving\nThese tasks can be done in the UI. They can also be automated by leveraging the Databricks Python SDK (AWS|Azure|GCP) available in Databricks Runtime 13.3LTS+","commandVersion":7,"state":"input","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"704ff066-1ac0-4a5a-ba00-4d29888a80d4"},{"version":"CommandV1","origId":4523734794182326,"guid":"7b57779d-98be-4e1f-a5e0-ebcb91417bde","subtype":"command","commandType":"auto","position":3.0,"command":"%md\n# Serve features with Databricks Lakebase's Online Tables (beta)\nTo serve prediction queries with low latency, publish the features to Databricks online tables and serve them in real time to the model.\n\nDuring the feature engineering step, we have created a Delta Table as an offline feature table. Recall that any Delta Table with a primary key can be a feature table in Databricks.","commandVersion":2,"state":"input","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"4282b03f-d7ea-4166-ae32-54ade883433d"},{"version":"CommandV1","origId":4523734794182329,"guid":"d56565f5-1ffe-4824-b786-a693c38282ce","subtype":"command","commandType":"auto","position":6.0,"command":"%sql\nALTER TABLE advanced_churn_feature_table SET TBLPROPERTIES (delta.enableChangeDataFeed = true)","commandVersion":12,"state":"finished","results":{"type":"listResults","data":[{"type":"mimeBundle","data":{"application/vnd.databricks.empty-table+json":{"directive_name":"NoDirective"}},"executionCount":null,"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"kernelSessionId":"771ef9ab-9715c0ad760be0c2d9f09d1f"}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":{"type":"baseError","stackFrames":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)","File \u001B[0;32m<command-4523734794182329>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mALTER TABLE\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n","File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:194\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    191\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    192\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    193\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n","File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:187\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    185\u001B[0m         query_text \u001B[38;5;241m=\u001B[39m sub_query\u001B[38;5;241m.\u001B[39mquery()\n\u001B[1;32m    186\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n\u001B[0;32m--> 187\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    191\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    192\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    193\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n","File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:213\u001B[0m, in \u001B[0;36mSqlMagic._handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n\u001B[1;32m    211\u001B[0m     query \u001B[38;5;241m=\u001B[39m sql_directive\u001B[38;5;241m.\u001B[39msql()\n\u001B[1;32m    212\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_udf_if_needed(query)\n\u001B[0;32m--> 213\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_query_request_result(query)\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m directive_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRemoveWidgetSpec\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetNotebookArguments()\u001B[38;5;241m.\u001B[39mremoveWidget(\n\u001B[1;32m    216\u001B[0m         argName\u001B[38;5;241m=\u001B[39msql_directive\u001B[38;5;241m.\u001B[39mname(), bindings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetCurrentBindings())\n","File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:151\u001B[0m, in \u001B[0;36mSqlMagic._get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(widget_bindings \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_widget_cache\u001B[38;5;241m.\u001B[39mvalues) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPARAM_SYNTAX_USAGE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 151\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39msql(query, widget_bindings)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/session.py:875\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m    872\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n\u001B[1;32m    874\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n\u001B[0;32m--> 875\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m    877\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[0;31mParseException\u001B[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input. SQLSTATE: 42601 (line 1, pos 11)\n\n== SQL ==\nALTER TABLE\n-----------^^^\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.parser.ParseException\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:478)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:119)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:163)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$7(SparkSession.scala:824)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$6(SparkSession.scala:824)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:675)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{},"jupyterProps":{"ename":"ParseException","evalue":"\n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input. SQLSTATE: 42601 (line 1, pos 11)\n\n== SQL ==\nALTER TABLE\n-----------^^^\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.parser.ParseException\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:478)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:119)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:163)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$7(SparkSession.scala:824)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$6(SparkSession.scala:824)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:675)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"},"sqlProps":{"sqlState":"42601","errorClass":"PARSE_SYNTAX_ERROR","stackTrace":"org.apache.spark.sql.catalyst.parser.ParseException\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:478)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:119)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:163)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$7(SparkSession.scala:824)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$6(SparkSession.scala:824)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:675)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)","startIndex":null,"stopIndex":null,"pysparkCallSite":"","pysparkFragment":"","pysparkSummary":"","breakingChangeInfo":null}},"workflows":[],"startTime":1761408907466,"submitTime":1761408907439,"finishTime":1761408909063,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000,"implicitDf":true},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["mimeBundle",null]],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"89693bc4-6a82-4905-9881-ce3914d33eb6"},{"version":"CommandV1","origId":4523734794182330,"guid":"fde67423-2442-4f2d-95c0-2cf43acc154a","subtype":"command","commandType":"auto","position":7.0,"command":"# Option 2: Use Databricks Feature Engineering SDK\n\nfrom databricks.feature_engineering import FeatureEngineeringClient\nfe = FeatureEngineeringClient()\n\n# Set online store name\nonline_store_name = dbutils.widgets.get(\"online_store_name\")\nonline_store = fe.get_online_store(name=online_store_name)\nonline_store","commandVersion":28,"state":"finished","results":{"type":"listResults","data":[],"arguments":{"online_store_name":"online_store_name"},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1761408919154,"submitTime":1761408919129,"finishTime":1761408919370,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"72cfae7e-2ad9-461a-a1ba-0e88e28a7762"},{"version":"CommandV1","origId":6245371248702132,"guid":"a45724a0-8657-494f-9cc3-0a82ccae951f","subtype":"command","commandType":"auto","position":8.0,"command":"from time import time\n\n\nif online_store:\n    print(f\"Online store exists\")\n    print(f\"Store: {online_store.name}, State: {online_store.state}, Capactiy:{online_store.capacity}\")\n\n    if dbutils.widgets.get(\"drop_online_store\")=='True' and not is_smoke_test:\n        fe.delete_online_store(name=online_store_name)\n        time.sleep(60)\n\n        print(f\"Dropping/Recreating it\")\n        online_store = fe.create_online_store(name=online_store_name, capacity=\"CU_1\")\n\nelif not online_store and not is_smoke_test:\n    print(f\"Creating online store: {online_store_name}\")\n    online_store = fe.create_online_store(name=online_store_name, capacity=\"CU_1\")\n\n# Note: enable lakebase to create Online Feature store else this will show error.","commandVersion":64,"state":"error","results":{"type":"listResults","data":[{"type":"ansi","data":"Creating online store: online_store_name\n","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":"<span class='ansi-red-fg'>BadRequest</span>: Online Feature Store requires Lakebase to be enabled on this workspace. Contact your workspace admin or Databricks support.","errorTraceType":"baseError","error":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mBadRequest\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-6245371248702132>, line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m online_store \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_smoke_test:\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating online store: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00monline_store_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 17\u001B[0m     online_store \u001B[38;5;241m=\u001B[39m fe\u001B[38;5;241m.\u001B[39mcreate_online_store(name\u001B[38;5;241m=\u001B[39monline_store_name, capacity\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCU_1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/feature_engineering/client.py:1821\u001B[0m, in \u001B[0;36mFeatureEngineeringClient.create_online_store\u001B[0;34m(self, name, capacity, read_replica_count)\u001B[0m\n\u001B[1;32m   1818\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m existing_store \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1819\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m AlreadyExists(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnline store with name \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m already exists\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1821\u001B[0m online_store \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workspace_client\u001B[38;5;241m.\u001B[39mfeature_store\u001B[38;5;241m.\u001B[39mcreate_online_store(\n\u001B[1;32m   1822\u001B[0m     online_store\u001B[38;5;241m=\u001B[39mOnlineStore(\n\u001B[1;32m   1823\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n\u001B[1;32m   1824\u001B[0m         capacity\u001B[38;5;241m=\u001B[39mcapacity,\n\u001B[1;32m   1825\u001B[0m         read_replica_count\u001B[38;5;241m=\u001B[39mread_replica_count,\n\u001B[1;32m   1826\u001B[0m     )\n\u001B[1;32m   1827\u001B[0m )\n\u001B[1;32m   1829\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatabricksOnlineStore\u001B[38;5;241m.\u001B[39m_from_online_store(online_store)\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/service/ml.py:6975\u001B[0m, in \u001B[0;36mFeatureStoreAPI.create_online_store\u001B[0;34m(self, online_store)\u001B[0m\n\u001B[1;32m   6969\u001B[0m body \u001B[38;5;241m=\u001B[39m online_store\u001B[38;5;241m.\u001B[39mas_dict()\n\u001B[1;32m   6970\u001B[0m headers \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   6971\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccept\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   6972\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   6973\u001B[0m }\n\u001B[0;32m-> 6975\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api\u001B[38;5;241m.\u001B[39mdo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/api/2.0/feature-store/online-stores\u001B[39m\u001B[38;5;124m\"\u001B[39m, body\u001B[38;5;241m=\u001B[39mbody, headers\u001B[38;5;241m=\u001B[39mheaders)\n\u001B[1;32m   6976\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m OnlineStore\u001B[38;5;241m.\u001B[39mfrom_dict(res)\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/core.py:85\u001B[0m, in \u001B[0;36mApiClient.do\u001B[0;34m(self, method, path, url, query, headers, body, raw, files, data, auth, response_headers)\u001B[0m\n\u001B[1;32m     83\u001B[0m     path \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m^/api/2.0/fs/files//\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/api/2.0/fs/files/\u001B[39m\u001B[38;5;124m\"\u001B[39m, path)\n\u001B[1;32m     84\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cfg\u001B[38;5;241m.\u001B[39mhost\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api_client\u001B[38;5;241m.\u001B[39mdo(\n\u001B[1;32m     86\u001B[0m     method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[1;32m     87\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m     88\u001B[0m     query\u001B[38;5;241m=\u001B[39mquery,\n\u001B[1;32m     89\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m     90\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m     91\u001B[0m     raw\u001B[38;5;241m=\u001B[39mraw,\n\u001B[1;32m     92\u001B[0m     files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[1;32m     93\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[1;32m     94\u001B[0m     auth\u001B[38;5;241m=\u001B[39mauth,\n\u001B[1;32m     95\u001B[0m     response_headers\u001B[38;5;241m=\u001B[39mresponse_headers,\n\u001B[1;32m     96\u001B[0m )\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/_base_client.py:199\u001B[0m, in \u001B[0;36m_BaseClient.do\u001B[0;34m(self, method, url, query, headers, body, raw, files, data, auth, response_headers)\u001B[0m\n\u001B[1;32m    196\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetry disabled for non-seekable stream: type=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    197\u001B[0m     call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_perform\n\u001B[0;32m--> 199\u001B[0m response \u001B[38;5;241m=\u001B[39m call(\n\u001B[1;32m    200\u001B[0m     method,\n\u001B[1;32m    201\u001B[0m     url,\n\u001B[1;32m    202\u001B[0m     query\u001B[38;5;241m=\u001B[39mquery,\n\u001B[1;32m    203\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    204\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m    205\u001B[0m     raw\u001B[38;5;241m=\u001B[39mraw,\n\u001B[1;32m    206\u001B[0m     files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[1;32m    207\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[1;32m    208\u001B[0m     auth\u001B[38;5;241m=\u001B[39mauth,\n\u001B[1;32m    209\u001B[0m )\n\u001B[1;32m    211\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m header \u001B[38;5;129;01min\u001B[39;00m response_headers \u001B[38;5;28;01mif\u001B[39;00m response_headers \u001B[38;5;28;01melse\u001B[39;00m []:\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/retries.py:59\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m         retry_reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(err)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is allowed to retry\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retry_reason \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;66;03m# raise if exception is not retryable\u001B[39;00m\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m     61\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetrying: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretry_reason\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (sleeping ~\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msleep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m before_retry:\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/retries.py:38\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m clock\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m<\u001B[39m deadline:\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 38\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m     40\u001B[0m         last_err \u001B[38;5;241m=\u001B[39m err\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/_base_client.py:301\u001B[0m, in \u001B[0;36m_BaseClient._perform\u001B[0;34m(self, method, url, query, headers, body, raw, files, data, auth)\u001B[0m\n\u001B[1;32m    299\u001B[0m error \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_parser\u001B[38;5;241m.\u001B[39mget_api_error(response)\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 301\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n\n\u001B[0;31mBadRequest\u001B[0m: Online Feature Store requires Lakebase to be enabled on this workspace. Contact your workspace admin or Databricks support.","errorDetails":null,"baseErrorDetails":{"type":"baseError","stackFrames":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mBadRequest\u001B[0m                                Traceback (most recent call last)","File \u001B[0;32m<command-6245371248702132>, line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m online_store \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_smoke_test:\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating online store: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00monline_store_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 17\u001B[0m     online_store \u001B[38;5;241m=\u001B[39m fe\u001B[38;5;241m.\u001B[39mcreate_online_store(name\u001B[38;5;241m=\u001B[39monline_store_name, capacity\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCU_1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n","File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/feature_engineering/client.py:1821\u001B[0m, in \u001B[0;36mFeatureEngineeringClient.create_online_store\u001B[0;34m(self, name, capacity, read_replica_count)\u001B[0m\n\u001B[1;32m   1818\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m existing_store \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1819\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m AlreadyExists(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnline store with name \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m already exists\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1821\u001B[0m online_store \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workspace_client\u001B[38;5;241m.\u001B[39mfeature_store\u001B[38;5;241m.\u001B[39mcreate_online_store(\n\u001B[1;32m   1822\u001B[0m     online_store\u001B[38;5;241m=\u001B[39mOnlineStore(\n\u001B[1;32m   1823\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n\u001B[1;32m   1824\u001B[0m         capacity\u001B[38;5;241m=\u001B[39mcapacity,\n\u001B[1;32m   1825\u001B[0m         read_replica_count\u001B[38;5;241m=\u001B[39mread_replica_count,\n\u001B[1;32m   1826\u001B[0m     )\n\u001B[1;32m   1827\u001B[0m )\n\u001B[1;32m   1829\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatabricksOnlineStore\u001B[38;5;241m.\u001B[39m_from_online_store(online_store)\n","File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/service/ml.py:6975\u001B[0m, in \u001B[0;36mFeatureStoreAPI.create_online_store\u001B[0;34m(self, online_store)\u001B[0m\n\u001B[1;32m   6969\u001B[0m body \u001B[38;5;241m=\u001B[39m online_store\u001B[38;5;241m.\u001B[39mas_dict()\n\u001B[1;32m   6970\u001B[0m headers \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   6971\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccept\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   6972\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   6973\u001B[0m }\n\u001B[0;32m-> 6975\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api\u001B[38;5;241m.\u001B[39mdo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/api/2.0/feature-store/online-stores\u001B[39m\u001B[38;5;124m\"\u001B[39m, body\u001B[38;5;241m=\u001B[39mbody, headers\u001B[38;5;241m=\u001B[39mheaders)\n\u001B[1;32m   6976\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m OnlineStore\u001B[38;5;241m.\u001B[39mfrom_dict(res)\n","File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/core.py:85\u001B[0m, in \u001B[0;36mApiClient.do\u001B[0;34m(self, method, path, url, query, headers, body, raw, files, data, auth, response_headers)\u001B[0m\n\u001B[1;32m     83\u001B[0m     path \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m^/api/2.0/fs/files//\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/api/2.0/fs/files/\u001B[39m\u001B[38;5;124m\"\u001B[39m, path)\n\u001B[1;32m     84\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cfg\u001B[38;5;241m.\u001B[39mhost\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api_client\u001B[38;5;241m.\u001B[39mdo(\n\u001B[1;32m     86\u001B[0m     method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[1;32m     87\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m     88\u001B[0m     query\u001B[38;5;241m=\u001B[39mquery,\n\u001B[1;32m     89\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m     90\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m     91\u001B[0m     raw\u001B[38;5;241m=\u001B[39mraw,\n\u001B[1;32m     92\u001B[0m     files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[1;32m     93\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[1;32m     94\u001B[0m     auth\u001B[38;5;241m=\u001B[39mauth,\n\u001B[1;32m     95\u001B[0m     response_headers\u001B[38;5;241m=\u001B[39mresponse_headers,\n\u001B[1;32m     96\u001B[0m )\n","File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/_base_client.py:199\u001B[0m, in \u001B[0;36m_BaseClient.do\u001B[0;34m(self, method, url, query, headers, body, raw, files, data, auth, response_headers)\u001B[0m\n\u001B[1;32m    196\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetry disabled for non-seekable stream: type=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    197\u001B[0m     call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_perform\n\u001B[0;32m--> 199\u001B[0m response \u001B[38;5;241m=\u001B[39m call(\n\u001B[1;32m    200\u001B[0m     method,\n\u001B[1;32m    201\u001B[0m     url,\n\u001B[1;32m    202\u001B[0m     query\u001B[38;5;241m=\u001B[39mquery,\n\u001B[1;32m    203\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    204\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m    205\u001B[0m     raw\u001B[38;5;241m=\u001B[39mraw,\n\u001B[1;32m    206\u001B[0m     files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[1;32m    207\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[1;32m    208\u001B[0m     auth\u001B[38;5;241m=\u001B[39mauth,\n\u001B[1;32m    209\u001B[0m )\n\u001B[1;32m    211\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m header \u001B[38;5;129;01min\u001B[39;00m response_headers \u001B[38;5;28;01mif\u001B[39;00m response_headers \u001B[38;5;28;01melse\u001B[39;00m []:\n","File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/retries.py:59\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m         retry_reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(err)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is allowed to retry\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retry_reason \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;66;03m# raise if exception is not retryable\u001B[39;00m\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m     61\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetrying: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretry_reason\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (sleeping ~\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msleep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m before_retry:\n","File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/retries.py:38\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m clock\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m<\u001B[39m deadline:\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 38\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m     40\u001B[0m         last_err \u001B[38;5;241m=\u001B[39m err\n","File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-aeb36191-ffc0-422c-a2fd-3c5267d26ce8/lib/python3.12/site-packages/databricks/sdk/_base_client.py:301\u001B[0m, in \u001B[0;36m_BaseClient._perform\u001B[0;34m(self, method, url, query, headers, body, raw, files, data, auth)\u001B[0m\n\u001B[1;32m    299\u001B[0m error \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_parser\u001B[38;5;241m.\u001B[39mget_api_error(response)\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 301\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n","\u001B[0;31mBadRequest\u001B[0m: Online Feature Store requires Lakebase to be enabled on this workspace. Contact your workspace admin or Databricks support."],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{},"jupyterProps":{"ename":"BadRequest","evalue":"Online Feature Store requires Lakebase to be enabled on this workspace. Contact your workspace admin or Databricks support."},"sqlProps":null},"workflows":[],"startTime":1761409284214,"submitTime":1761409284180,"finishTime":1761409286196,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["ansi",41]],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"535a305d-0952-4407-b505-dfd909d2a08a"},{"version":"CommandV1","origId":4523734794182328,"guid":"0b3d5b8c-be4c-497f-928d-fcde0ea469d6","subtype":"command","commandType":"auto","position":5.0,"command":"%md\n# Create Databricks Online Feature Store\n\n#### Enable Change-Data-Feed on Feature Table for performance considerations\nAn online table is a read-only copy of a Delta Table stored in row-oriented format optimized for online access.\n\nDatabricks allows the online tables to be refreshed efficiently whenever there are updates to the underlying feature tables. This is enabled through Delta Lake's Change Data Feed feature. Let us first enable Change Data Feed on the underlying feature table churn_feature_table.","commandVersion":8,"state":"input","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"86280eb7-2262-4247-97b5-438c5b06d703"},{"version":"CommandV1","origId":4523734794182327,"guid":"8fac944a-c584-4f58-a5b5-88f99eead5b9","subtype":"command","commandType":"auto","position":4.0,"command":"is_smoke_test = dbutils.widgets.get(\"smoke_test\") == \"True\"","commandVersion":7,"state":"finished","results":{"type":"listResults","data":[],"arguments":{"smoke_test":"False"},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1761408903256,"submitTime":1761408903230,"finishTime":1761408903355,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"4e667f76-6fc1-4d3f-b45e-7702ade2b3dd"},{"version":"CommandV1","origId":4523734794182322,"guid":"3d57e78b-27fd-44d8-9eaf-9ab3fb517c78","subtype":"command","commandType":"auto","position":1.0,"command":"%pip install --quiet databricks-feature-engineering>=0.13.0a8 mlflow --upgrade\n\n%restart_python","commandVersion":9,"state":"finished","results":{"type":"listResults","data":[{"type":"ansi","data":"\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1761408863104,"submitTime":1761408863021,"finishTime":1761408869709,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["ansi",132]],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"b1316103-e181-490c-b025-54418433adf0"},{"version":"CommandV1","origId":4523734794182325,"guid":"53f503f3-a0ba-478c-9b70-0ab9202975f8","subtype":"command","commandType":"auto","position":2.0,"command":"dbutils.widgets.text(\"model_name\", f\"sai_datastorage.default.advanced_mlops_churn\", \"Model Name\")\ndbutils.widgets.text(\"model_version\", \"1\", \"Model Version\")\ndbutils.widgets.text(\"online_store_name\", \"mlops_churn_advanced\", \"Online Store Name\")\ndbutils.widgets.dropdown(\"drop_online_store\", \"False\", [\"True\", \"False\"], \"Reset Online Table(s)\")\ndbutils.widgets.dropdown(\"smoke_test\", \"False\", [\"True\", \"False\"], \"Smoke Test Flag\")","commandVersion":60,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{"smoke_test":{"widgetType":"dropdown","name":"smoke_test","defaultValue":"False","label":"Smoke Test Flag","options":{"widgetType":"dropdown","choices":["True","False"],"autoCreated":null}},"model_version":{"widgetType":"text","name":"model_version","defaultValue":"1","label":"Model Version","options":{"widgetType":"text","validationRegex":null,"autoCreated":null}},"online_store_name":{"widgetType":"text","name":"online_store_name","defaultValue":"mlops_churn_advanced","label":"Online Store Name","options":{"widgetType":"text","validationRegex":null,"autoCreated":null}},"drop_online_store":{"widgetType":"dropdown","name":"drop_online_store","defaultValue":"False","label":"Reset Online Table(s)","options":{"widgetType":"dropdown","choices":["True","False"],"autoCreated":null}},"model_name":{"widgetType":"text","name":"model_name","defaultValue":"sai_datastorage.default.advanced_mlops_churn","label":"Model Name","options":{"widgetType":"text","validationRegex":null,"autoCreated":null}}},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1761408892769,"submitTime":1761408892741,"finishTime":1761408892947,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"84287142-96a4-4e9a-868f-95222911c399"}],"dashboards":[],"guid":"f3e55298-572c-47e1-ab72-1dcbefbf0ffa","globalVars":{},"iPythonMetadata":null,"inputWidgets":{"smoke_test":{"nuid":"823c38e8-7ea4-47be-a6a6-8cd7b85787a7","currentValue":"False","widgetInfo":{"widgetType":"dropdown","name":"smoke_test","defaultValue":"False","label":"Smoke Test Flag","options":{"widgetType":"dropdown","choices":["True","False"],"autoCreated":null}},"typedWidgetInfo":{"name":"smoke_test","parameterDataType":"String","label":"Smoke Test Flag","defaultValue":"False","options":{"widgetDisplayType":"Dropdown","choices":["True","False"],"fixedDomain":true,"multiselect":false},"autoCreated":false}},"model_version":{"nuid":"6d0ca84b-35fb-48e4-8a54-4660ad835cd9","currentValue":"1","widgetInfo":{"widgetType":"text","name":"model_version","defaultValue":"1","label":"Model Version","options":{"widgetType":"text","validationRegex":null,"autoCreated":null}},"typedWidgetInfo":{"name":"model_version","parameterDataType":"String","label":"Model Version","defaultValue":"1","options":{"widgetDisplayType":"Text","validationRegex":null},"autoCreated":false}},"online_store_name":{"nuid":"13ae2889-61ab-477f-bb2f-dca9adc85c82","currentValue":"online_store_name","widgetInfo":{"widgetType":"text","name":"online_store_name","defaultValue":"mlops_churn_advanced","label":"Online Store Name","options":{"widgetType":"text","validationRegex":null,"autoCreated":null}},"typedWidgetInfo":{"name":"online_store_name","parameterDataType":"String","label":"Online Store Name","defaultValue":"mlops_churn_advanced","options":{"widgetDisplayType":"Text","validationRegex":null},"autoCreated":false}},"drop_online_store":{"nuid":"d75f7885-3ebb-4157-8165-19acf753b5f1","currentValue":"False","widgetInfo":{"widgetType":"dropdown","name":"drop_online_store","defaultValue":"False","label":"Reset Online Table(s)","options":{"widgetType":"dropdown","choices":["True","False"],"autoCreated":null}},"typedWidgetInfo":{"name":"drop_online_store","parameterDataType":"String","label":"Reset Online Table(s)","defaultValue":"False","options":{"widgetDisplayType":"Dropdown","choices":["True","False"],"fixedDomain":true,"multiselect":false},"autoCreated":false}},"model_name":{"nuid":"9e8de4a4-28d2-4639-af70-c3e71bde924e","currentValue":"sai_datastorage.default.advanced_mlops_churn","widgetInfo":{"widgetType":"text","name":"model_name","defaultValue":"sai_datastorage.default.advanced_mlops_churn","label":"Model Name","options":{"widgetType":"text","validationRegex":null,"autoCreated":null}},"typedWidgetInfo":{"name":"model_name","parameterDataType":"String","label":"Model Name","defaultValue":"sai_datastorage.default.advanced_mlops_churn","options":{"widgetDisplayType":"Text","validationRegex":null},"autoCreated":false}}},"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":4523734794182329,"dataframes":["_sqldf"]}},"reposExportFormat":"JUPYTER","environmentMetadata":{"client":"4","base_environment":""},"computePreferences":{"hardware":{"accelerator":null,"gpuPoolId":null,"memory":null}},"inputWidgetPreferences":null}